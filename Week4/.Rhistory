y
theta.star <- numeric(100)
x <- numeric()
set.seed(343)
for (i in 1:100){
dat <- gen_dat()
x[i] <- mean(dat$W1)
Y1 <- rnorm(n, mean = 4.4*1 + 0.7*dat$W1 - 2*1*dat$W2 -2*1*dat$C,
sd = 0.3)
Y0 <- rnorm(n, mean = 4.4*0 + 0.7*dat$W1 - 2*0*dat$W2 -2*0*dat$C,
sd = 0.3)
theta.star[i] <- mean(Y1) - mean(Y0)
}
mean(theta.star)
phi <- numeric(100)
y <- numeric()
set.seed(343)
for (i in 1:100){
dat <- gen_dat()
y[i] <- mean(dat$W1)
dat %>%
group_by(W1, W2, A) %>%
summarise(EY = mean(Y)) %>%
group_by(W1, W2) %>%
summarise(phi = diff(EY)) %>%
pull(phi) %>%
mean()-> phi[i]
}
mean(phi)
sum(x!=y)
theta.star <- numeric(100)
x <- numeric()
set.seed(343)
for (i in 1:100){
dat <- gen_dat()
x[i] <- mean(dat$W1)
Y1 <- rnorm(n, mean = 4.4*1 + 0.7*dat$W1 - 2*1*dat$W2 -2*1*dat$C,
sd = 0.3)
Y0 <- rnorm(n, mean = 4.4*0 + 0.7*dat$W1 - 2*0*dat$W2 -2*0*dat$C,
sd = 0.3)
theta.star[i] <- mean(Y1) - mean(Y0)
}
mean(theta.star)
phi <- numeric(100)
y <- numeric()
# set.seed(343)
for (i in 1:100){
dat <- gen_dat()
y[i] <- mean(dat$W1)
dat %>%
group_by(W1, W2, A) %>%
summarise(EY = mean(Y)) %>%
group_by(W1, W2) %>%
summarise(phi = diff(EY)) %>%
pull(phi) %>%
mean()-> phi[i]
}
mean(phi)
sum(x!=y)
phi <- numeric(100)
set.seed(343)
for (i in 1:100){
dat <- gen_dat()
dat %>%
group_by(W1, W2, A) %>%
summarise(EY = mean(Y)) %>%
group_by(W1, W2) %>%
summarise(phi = diff(EY)) %>%
pull(phi) %>%
mean()-> phi[i]
Y1 <- rnorm(n, mean = 4.4*1 + 0.7*dat$W1 - 2*1*dat$W2 -2*1*dat$C,
sd = 0.3)
Y0 <- rnorm(n, mean = 4.4*0 + 0.7*dat$W1 - 2*0*dat$W2 -2*0*dat$C,
sd = 0.3)
theta.star[i] <- mean(Y1) - mean(Y0)
}
mean(phi)
mean(theta.star)
mean(theta.star)
apply(estimates, 2, mean)
apply(estimates, 2, mean)  -> x
class(x)
names(x)
colnames(x)
row(x)
rownames(x)
x
x %>%
kable()
apply(estimates, 2, mean)  -> y
y %>%
kable()
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
# lda, qda
library(MASS)
# knn
library(class)
library(dplyr)
# table
library(knitr)
library(kableExtra)
error.wrong
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = F)
# lda, qda
library(MASS)
# knn
library(class)
library(dplyr)
# table
library(knitr)
library(kableExtra)
bc <- read.csv("breast-cancer-wisconsin.data", header = FALSE)
colnames(bc) <- c("id", "Clump_thickness", "Uniform_cell_size",
"Uniform_cell_shape", "Marginal_Adhesion",
"Single_epithelial_cell_size",
"Bare_Nuclei", "Bland)Chromatin",
"Normal_nucleoli", "Mitoses", "Class")
bc$Class = ifelse(bc$Class == 2, 0, 1)
bc$Bare_Nuclei <- as.numeric(bc$Bare_Nuclei)
bc <- bc[complete.cases(bc), -1]
Knn_err <- function(k, training, test, c){
knn.pre <- knn(training[, -c], test[, -c], cl = training[, c], k)
mean(knn.pre != test[, c])
}
k=10
folds=sample(1:k,nrow(bc),replace =TRUE)
cv.errors=matrix(NA,k,4)
colnames(cv.errors) <- c("logistic regression", "lda", "qda", "k-nn")
for(j in 1:k){
# logistic regression
logi.fit <- glm(Class ~ ., family = binomial,
data = bc[folds != j, ])
logi.pre <- predict(logi.fit, bc[folds == j, ], type = "response")
logi.pre <- ifelse(logi.pre > 0.5, 1, 0)
cv.errors[j, 1] <- mean(logi.pre != bc$Class[folds == j])
# lda
lda.fit <- lda(Class ~ ., data = bc[folds != j, ])
lda.pre <- predict(lda.fit, bc[folds == j, ],
type = "response")$class
cv.errors[j, 2] <- mean(lda.pre != bc$Class[folds == j])
# qda
qda.fit <- qda(Class ~ ., data = bc[folds != j, ])
qda.pre <- predict(qda.fit, bc[folds == j, ],
type = "response")$class
cv.errors[j, 3] <- mean(qda.pre != bc$Class[folds == j])
# k-nn
knn.ans <- sapply(1:10, Knn_err, bc[folds != j, ],
bc[folds == j, ], 10)
cv.errors[j, 4] <- min(knn.ans)
#cv.errors[j, 5] <- which.min(knn.ans)
}
apply(cv.errors, 2, mean) %>%
kable() %>%
kable_styling()
knn.ans
k=10
folds=sample(1:k,nrow(bc),replace =TRUE)
cv.errors=matrix(NA,k,5)
colnames(cv.errors) <- c("logistic regression", "lda", "qda", "k-nn")
k=10
folds=sample(1:k,nrow(bc),replace =TRUE)
cv.errors=matrix(NA,k,5)
colnames(cv.errors) <- c("logistic regression", "lda", "qda", "k-nn", "k")
for(j in 1:k){
# logistic regression
logi.fit <- glm(Class ~ ., family = binomial,
data = bc[folds != j, ])
logi.pre <- predict(logi.fit, bc[folds == j, ], type = "response")
logi.pre <- ifelse(logi.pre > 0.5, 1, 0)
cv.errors[j, 1] <- mean(logi.pre != bc$Class[folds == j])
# lda
lda.fit <- lda(Class ~ ., data = bc[folds != j, ])
lda.pre <- predict(lda.fit, bc[folds == j, ],
type = "response")$class
cv.errors[j, 2] <- mean(lda.pre != bc$Class[folds == j])
# qda
qda.fit <- qda(Class ~ ., data = bc[folds != j, ])
qda.pre <- predict(qda.fit, bc[folds == j, ],
type = "response")$class
cv.errors[j, 3] <- mean(qda.pre != bc$Class[folds == j])
# k-nn
knn.ans <- sapply(1:10, Knn_err, bc[folds != j, ],
bc[folds == j, ], 10)
cv.errors[j, 4] <- min(knn.ans)
cv.errors[j, 5] <- which.min(knn.ans)
}
apply(cv.errors, 2, mean) %>%
kable() %>%
kable_styling()
cv.errors
apply(cv.errors[, -5], 2, mean) %>%
kable() %>%
kable_styling()
cv.errors[, 4:%]
cv.errors[, 4:5]
cv.errors[, 4:5] %>%
kable() %>%
kable_styling()
cv.errors
knn.ans
k=10
folds=sample(1:k,nrow(bc),replace =TRUE)
cv.errors=matrix(NA,k,3)
knn.ans <- matrix(NA, k, 10)
colnames(cv.errors) <- c("logistic regression", "lda", "qda")
for(j in 1:k){
# logistic regression
logi.fit <- glm(Class ~ ., family = binomial,
data = bc[folds != j, ])
logi.pre <- predict(logi.fit, bc[folds == j, ], type = "response")
logi.pre <- ifelse(logi.pre > 0.5, 1, 0)
cv.errors[j, 1] <- mean(logi.pre != bc$Class[folds == j])
# lda
lda.fit <- lda(Class ~ ., data = bc[folds != j, ])
lda.pre <- predict(lda.fit, bc[folds == j, ],
type = "response")$class
cv.errors[j, 2] <- mean(lda.pre != bc$Class[folds == j])
# qda
qda.fit <- qda(Class ~ ., data = bc[folds != j, ])
qda.pre <- predict(qda.fit, bc[folds == j, ],
type = "response")$class
cv.errors[j, 3] <- mean(qda.pre != bc$Class[folds == j])
# k-nn
knn.ans[, j] <- sapply(1:10, Knn_err, bc[folds != j, ],
bc[folds == j, ], 10)
}
knn.ans
cv.errors
apply(cv.errors[, -5], 2, mean) %>%
kable() %>%
kable_styling()
apply(knn.ans, 2, mean)
data.frame(k = 1:10, error.rate = apply(knn.ans, 2, mean)) %>%
kable() %>%
kable_styling()
apply(cv.errors[, -5], 2, mean)
data.frame("Method" = apply(cv.errors[, -5], 2, mean)) %>%
kable() %>%
kable_styling()
data.frame(k = 1:10, error.rate = apply(knn.ans, 2, mean))
data.frame(k = 1:10, error.rate = apply(knn.ans, 2, mean)) %>%
kable() %>%
kable_styling()
which.min(knn.ans$error.rate)
knn.ans
k=10
folds=sample(1:k,nrow(bc),replace =TRUE)
cv.errors=matrix(NA,k,3)
knn.ans <- matrix(NA, k, 10)
colnames(cv.errors) <- c("logistic regression", "lda", "qda")
for(j in 1:k){
# logistic regression
logi.fit <- glm(Class ~ ., family = binomial,
data = bc[folds != j, ])
logi.pre <- predict(logi.fit, bc[folds == j, ], type = "response")
logi.pre <- ifelse(logi.pre > 0.5, 1, 0)
cv.errors[j, 1] <- mean(logi.pre != bc$Class[folds == j])
# lda
lda.fit <- lda(Class ~ ., data = bc[folds != j, ])
lda.pre <- predict(lda.fit, bc[folds == j, ],
type = "response")$class
cv.errors[j, 2] <- mean(lda.pre != bc$Class[folds == j])
# qda
qda.fit <- qda(Class ~ ., data = bc[folds != j, ])
qda.pre <- predict(qda.fit, bc[folds == j, ],
type = "response")$class
cv.errors[j, 3] <- mean(qda.pre != bc$Class[folds == j])
# k-nn
knn.ans[, j] <- sapply(1:10, Knn_err, bc[folds != j, ],
bc[folds == j, ], 10)
}
knn.ans <- data.frame(k = 1:10, error.rate = apply(knn.ans, 2, mean))
knn.ans %>%
kable() %>%
kable_styling()
k=10
folds=sample(1:k,nrow(bc),replace =TRUE)
cv.errors=matrix(NA,k,3)
knn.ans <- matrix(NA, k, 10)
colnames(cv.errors) <- c("logistic regression", "lda", "qda")
set.seed(132421)
for(j in 1:k){
# logistic regression
logi.fit <- glm(Class ~ ., family = binomial,
data = bc[folds != j, ])
logi.pre <- predict(logi.fit, bc[folds == j, ], type = "response")
logi.pre <- ifelse(logi.pre > 0.5, 1, 0)
cv.errors[j, 1] <- mean(logi.pre != bc$Class[folds == j])
# lda
lda.fit <- lda(Class ~ ., data = bc[folds != j, ])
lda.pre <- predict(lda.fit, bc[folds == j, ],
type = "response")$class
cv.errors[j, 2] <- mean(lda.pre != bc$Class[folds == j])
# qda
qda.fit <- qda(Class ~ ., data = bc[folds != j, ])
qda.pre <- predict(qda.fit, bc[folds == j, ],
type = "response")$class
cv.errors[j, 3] <- mean(qda.pre != bc$Class[folds == j])
# k-nn
knn.ans[, j] <- sapply(1:10, Knn_err, bc[folds != j, ],
bc[folds == j, ], 10)
}
data.frame("error.rate" = apply(cv.errors[, -5], 2, mean)) %>%
kable() %>%
kable_styling()
k=10
folds=sample(1:k,nrow(bc),replace =TRUE)
cv.errors=matrix(NA,k,3)
knn.ans <- matrix(NA, k, 10)
colnames(cv.errors) <- c("logistic regression", "lda", "qda")
for(j in 1:k){
# logistic regression
logi.fit <- glm(Class ~ ., family = binomial,
data = bc[folds != j, ])
logi.pre <- predict(logi.fit, bc[folds == j, ], type = "response")
logi.pre <- ifelse(logi.pre > 0.5, 1, 0)
cv.errors[j, 1] <- mean(logi.pre != bc$Class[folds == j])
# lda
lda.fit <- lda(Class ~ ., data = bc[folds != j, ])
lda.pre <- predict(lda.fit, bc[folds == j, ],
type = "response")$class
cv.errors[j, 2] <- mean(lda.pre != bc$Class[folds == j])
# qda
qda.fit <- qda(Class ~ ., data = bc[folds != j, ])
qda.pre <- predict(qda.fit, bc[folds == j, ],
type = "response")$class
cv.errors[j, 3] <- mean(qda.pre != bc$Class[folds == j])
# k-nn
knn.ans[, j] <- sapply(1:10, Knn_err, bc[folds != j, ],
bc[folds == j, ], 10)
}
data.frame("error.rate" = apply(cv.errors[, -5], 2, mean)) %>%
kable() %>%
kable_styling()
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(knitr)
library(kableExtra)
# data manipulation
library(tidyverse)
# Draw ROC plot
library(ROCR)
library(pROC)
library(simr)
load("../Data/ESP_reg.RData")
ESP.reg
load("../Data/ESP.RData")
ESP.reg
ESP
ESP$health_status_1 <- relevel(ESP$health_status_1, ref = "Not Poor")
full1 <- glm(health_status_1 ~ numeracy + educ + hisp + race +
income.cat + educ:numeracy, data = ESP,
family = "binomial")
final1 <- step(full1, direction = "both", trace = 0)
final1
multi_summary1 <- exp(cbind(coef(final1), confint(final1))[-1, ]) %>% as.data.frame()
colnames(multi_summary1)[1] <- "OR"
multi_summary1$sig <- ifelse(summary(final1)$coefficients[-1, 4] < 0.05, "*", "")
multi_summary1
# ROC curve
rocplot <- function(truth, pred, ...){
predob = prediction(pred, truth)
perf = performance(predob, "tpr", "fpr")
plot(perf, colorize=F, ...)
area = auc(truth, pred)
area = format(round(area, 4), nsmall = 4)
text(x = .8, y = .1, labels = paste("AUC = ", area))
segments(x0 = 0, y0 = 0, x1 = 1, y1 = 1, col = "gray", lty = 2)
}
rocplot(ESP$health_status_1, predict.glm(final1, ESP, type = "response"), title = "ROC plot")
title(main = "ROC plot")
ESP <- ESP.reg
ESP$health_status_1 <- relevel(ESP$health_status_1, ref = "Not Poor")
full1 <- glm(health_status_1 ~ numeracy + educ + hisp + race +
income.cat + educ:numeracy, data = ESP,
family = "binomial")
final1 <- step(full1, direction = "both", trace = 0)
multi_summary1 <- exp(cbind(coef(final1), confint(final1))[-1, ]) %>% as.data.frame()
colnames(multi_summary1)[1] <- "OR"
multi_summary1$sig <- ifelse(summary(final1)$coefficients[-1, 4] < 0.05, "*", "")
# power calculation
powerSim(final1, nsim = 100,  progress = FALSE)
kable(multi_summary1) %>%
kable_styling()
multi_summary1
multi_summary1
ESP
multi_summary1
ESP$hisp
sum(is.na(ESP))
ESP$hisp
sum(ESP$hisp == "Refused")
sum(ESP$income.cat == "$100k-$150k")
sum(ESP$income.cat == "$150k+")
View(multi_summary1)
View(multi_summary1)
summary(final1)$coefficients
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
options(scipen = 3, digits = 3)
PIAAC_result1
PIAAC_result1 <- data.frame(PIAAC_h1_1, PIAAC_h2_1) %>% t() %>% as.data.frame()
PIAAC_h1_0 <- c("0.7469","99.00% (94.55, 99.97)","\\*","","\\*","\\*")
PIAAC_h2_0 <- c("0.7098","100.0%(96.38,100.0)","\\*","\\*","\\*","\\*")
PIAAC_result0 <- data.frame(PIAAC_h1_0, PIAAC_h2_0) %>% t() %>% as.data.frame()
rownames(PIAAC_result0) <- c("Not Poor vs Poor", "Good vs Not Good")
colnames(PIAAC_result0) <- c("AUC", "Power(95%CI)", "numeracy", "educ", "income", "race")
PIAAC_result0 %>%
kable(caption = "PIAAC's unweighted regression summary with numeracy as the only predictor") %>%
kable_styling() %>%
add_header_above(c(" "=1,"Performance"=2,"Predictor"=1,"Covariates"=3))
PIAAC_h1_1 <- c("0.8198","100.0%(96.38,100.0)","\\*","\\*","","\\*","\\*","\\*")
PIAAC_h2_1 <- c("0.6907","54.00%(43.74,64.02)","","\\*","","\\*","\\*","")
PIAAC_result1 <- data.frame(PIAAC_h1_1, PIAAC_h2_1) %>% t() %>% as.data.frame()
rownames(PIAAC_result1) <- c("Not Poor vs Poor", "Good vs Not Good")
colnames(PIAAC_result1) <- c("AUC", "Power(95%CI)", "numeracy", "problem", "literacy", "educ", "income", "race")
PIAAC_result1 %>%
kable(caption = "PIAAC's unweighted regression summary with additional predictors") %>%
kable_styling() %>%
add_header_above(c(" "=1,"Performance"=2,"Predictor"=3,"Covariates"=3))
PIAAC_result1
ESP
wt.edu <- c(0.3892, 0.2777, 0.3332)
wt.income <- c(.0844, .2459, .418, .1233, .1283)
wt.race <- c(.697, .176, .091, .001, .026)
library(table1)
table1(ESP)
names(ESP)
table1(~ educ + income.cat + race + hisp, ESP)
ESP %>%
mutate(educ = relevel(educ, level = c("")))
ESP %>%
mutate(educ = relevel(educ, level = c("High school or less", "Some college or associate's degree", "Bachelor's degree or higher")))
?relevel
ESP %>%
mutate(educ = level(educ, level = c("High school or less", "Some college or associate's degree", "Bachelor's degree or higher")))
ESP %>%
mutate(educ = factor(educ, level = c("High school or less", "Some college or associate's degree", "Bachelor's degree or higher")))
ESP$income.cat
ESP$educ
ESP.weighted <- ESP %>%
filter(hisp != "Refused" & educ != "Refused") %>%
mutate(hisp = factor(hisp), educ = factor(educ),
income.cat = factor(income.cat), wt = 1)
ESP.weighted$educ
?relevel
table1(~ educ + income.cat + race + hisp, ESP.weighted)
# ESP.weighted <- surv.wt(ESP.weighted, wt.hisp, c1 = 7, c2 = "hisp")
.2459 + .0844
table1(~ educ + income.cat + race + hisp, ESP.weighted)
# ESP.weighted <- surv.wt(ESP.weighted, wt.hisp, c1 = 7, c2 = "hisp")
sum(c(.091, .001, .026))
# ESP.weighted <- surv.wt(ESP.weighted, wt.hisp, c1 = 7, c2 = "hisp")
sum(c(.091, .001, .026, .697, .176))
ESP.weighted
# ESP.weighted <- surv.wt(ESP.weighted, wt.hisp, c1 = 7, c2 = "hisp")
names(ESP.weighted)
ESP.weighted <- surv.wt(ESP.weighted, wt.edu, 5, "educ")
ESP.weighted <- ESP %>%
filter(hisp != "Refused" & educ != "Refused") %>%
mutate(hisp = factor(hisp), educ = factor(educ),
income.cat = factor(income.cat), wt = 1)
# This function generate survey weights automatically
surv.wt <- function(x, w, col_pos, col_name){
# x: dataset of interest
# w: NYS percentages
# cpl_pos: column number of the variable to be weighted
# col_name: name of the variable to be weighted
x1 <- x %>%
group_by_at(col_pos) %>%
summarise(wt = sum(wt)/nrow(x)) %>%
mutate(wt_new = w/wt) %>%
select(-wt)
merge(x, x1, by = col_name) %>%
mutate(wt = wt * wt_new) %>%
select(-wt_new)
}
?relevel
ESP.weighted$educ
# Manully input the NYS education level proportion and hispanic proportion
wt.educ <- c(0.3892, 0.2777, 0.3332)
wt.income <- c(0.33, .418, .1233, .1283)
wt.race <- c(.697, .176, 0.118)
ESP.weighted <- surv.wt(ESP.weighted, wt.edu, 5, "educ")
# x: dataset of interest
# w: NYS percentages
# cpl_pos: column number of the variable to be weighted
# col_name: name of the variable to be weighted
x <- ESP.weighted; w <- wt.edu; col_pos<- 5; col_name <- "educ"
x1 <- x %>%
group_by_at(col_pos) %>%
summarise(wt = sum(wt)/nrow(x)) %>%
mutate(wt_new = w/wt) %>%
select(-wt)
x %>%
group_by_at(col_pos) %>%
summarise(wt = sum(wt)/nrow(x)) %>%
mutate(wt_new = w/wt)
x %>%
group_by_at(col_pos) %>%
summarise(wt = sum(wt)/nrow(x)) %>%
mutate(wt_new = w/wt) %>%
select(-wt)
# x: dataset of interest
# w: NYS percentages
# cpl_pos: column number of the variable to be weighted
# col_name: name of the variable to be weighted
#x <- ESP.weighted; w <- wt.edu; col_pos<- 5; col_name <- "educ"
library(tidyverse)
x1 <- x %>%
group_by_at(col_pos) %>%
summarise(wt = sum(wt)/nrow(x)) %>%
mutate(wt_new = w/wt) %>%
select(-wt)
